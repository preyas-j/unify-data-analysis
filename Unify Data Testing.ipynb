{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d463eb-c5c8-4157-95aa-734f770e7f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**install_df**: get all unique installation_id to create installation_list that will later be iterated though to get relevant data for all installations \\\n",
    "**df**: get all BIN_UPDATE ASLog entries where bin mode is PORT \\\n",
    "**timestamp_df**: get local timestamp for each BIN_UPDATE where bin mode is PORT and convert that timestamp to a day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a285cca0-157f-4ec3-8367-c840125aefa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ready DFs"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Installation Data\").getOrCreate()\n",
    "\n",
    "install_df = spark.sql(\"\"\"\n",
    "                select distinct installation_id\n",
    "                from test_unify_analytics.bronze.logs_partition_1\n",
    "            \"\"\")\n",
    "# display(install_df)\n",
    "installation_list = install_df.toPandas()['installation_id'].tolist()\n",
    "# print(len(installation_list))\n",
    "\n",
    "# tag == 242 --> BIN_UPDATE\n",
    "# values[5] == 14 --> bin in PORT\n",
    "# df = spark.sql(\"\"\"\n",
    "#             select installation_id, tag, unix_timestamp, values\n",
    "#             from test_unify_analytics.bronze.logs_partition_1\n",
    "#             where tag = 242 AND values[5] = 14\n",
    "#         \"\"\")\n",
    "\n",
    "# tag == 249 --> PORT_CLOSEBIN\n",
    "df = spark.sql(\"\"\"\n",
    "            select installation_id, tag, local_installation_timestamp, unix_timestamp, values, values[0] as BinNo\n",
    "            from test_unify_analytics.bronze.logs_partition_1\n",
    "            where tag = 249\n",
    "        \"\"\")\n",
    "\n",
    "# values[5] == 14 --> bin in PORT\n",
    "# timestamp_df = spark.sql(\"\"\"\n",
    "#                     select local_installation_timestamp, values[0] as BinNo\n",
    "#                     from test_unify_analytics.bronze.logs_partition_1\n",
    "#                     where tag = 242 AND values[5] = 14\n",
    "#                 \"\"\")\n",
    "# timestamp_df = timestamp_df.withColumn('dotw', date_format(col('local_installation_timestamp'), 'EEEE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0a0556-c1be-46a9-a49f-ec07f5308afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "iterate through **installation_list** to filter **df** by a specific **installation_id** and create a corresponding CSV file for each installation_id that displays the average time between presentations in seconds and minutes for each bin in that installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec8c1ef-f813-49aa-a163-6d8671656d00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "avg bin times loop"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "dbutils.fs.mkdirs('dbfs:/FileStore/tables/avg_bin_time')\n",
    "\n",
    "df_bintimes = df.select(['installation_id', 'unix_timestamp', 'values'])\n",
    "df_bintimes.cache().count()\n",
    "for installation in installation_list:\n",
    "    df_loop = df_bintimes.filter((col('installation_id') == installation)) \n",
    "\n",
    "    df_loop = df_loop.withColumn('bin num', col('values')[0]) \\\n",
    "                     .withColumn('unix timestamp', col('unix_timestamp'))\n",
    "    window_spec = Window.partitionBy('bin num').orderBy('unix_timestamp')\n",
    "    df_loop = df_loop.withColumn('prev_timestamp', lag('unix_timestamp').over(window_spec)) \\\n",
    "                     .withColumn('time diff (sec)', col('unix_timestamp') - col('prev_timestamp'))\n",
    "\n",
    "    df_loop = df_loop.groupBy('bin num').agg(round(mean('time diff (sec)')).alias('avg time between presentations (sec)')).orderBy('avg time between presentations (sec)')\n",
    "    df_loop = df_loop.withColumn('avg time between presentations (min)', round(col('avg time between presentations (sec)') / 60))\n",
    "            \n",
    "    pd = df_loop.toPandas() #.sort_values(by=['avg time between presentations (sec)'])\n",
    "    # display(pd)\n",
    "\n",
    "    store_filepath = '/dbfs/FileStore/tables/avg_bin_time/' + installation + '.csv'\n",
    "    pd.to_csv(store_filepath)\n",
    "\n",
    "df_bintimes.unpersist()\n",
    "files = dbutils.fs.ls('/FileStore/tables/avg_bin_time') \n",
    "print('num files: ', len([f.name for f in files]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1c6bae-2298-4010-a01a-7084826c4e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "iterate through each installation in **installation_list** to filter **timestamp_df** in order to determine the number of presentations for each bin on each day of the week throughout the entire timespan of the data in the table for each individual installation and save the results to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfaf906-7e58-45de-ab75-ac2a125d2b4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dotw logic"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "dbutils.fs.mkdirs('dbfs:/FileStore/tables/dotw')\n",
    "\n",
    "timestamp_df = df.select(['installation_id', 'local_installation_timestamp', 'BinNo'])\n",
    "timestamp_df = timestamp_df.withColumn('dotw', date_format(col('local_installation_timestamp'), 'EEEE'))\n",
    "timestamp_df.cache().count()\n",
    "for installation in installation_list:\n",
    "    df_dotw = timestamp_df.filter((col('installation_id') == installation)).groupBy(['BinNo', 'dotw']).count()\n",
    "    # timestamp_df = timestamp_df.groupBy(['BinNo', 'dotw']).count()\n",
    "    pd_dotw = df_dotw.orderBy(desc('BinNo'), desc('count')).toPandas()\n",
    "\n",
    "    store_filepath = '/dbfs/FileStore/tables/dotw/' + installation + '.csv'\n",
    "    pd_dotw.to_csv(store_filepath)\n",
    "\n",
    "timestamp_df.unpersist()\n",
    "files = dbutils.fs.ls('/FileStore/tables/dotw') \n",
    "print('num files: ', len([f.name for f in files]))\n",
    "\n",
    "# time_list = timestamp_df.toPandas()['local_installation_timestamp'].tolist()\n",
    "# print(time_list[0].day_name())\n",
    "# for i, val in enumerate(time_list):\n",
    "#     time_list[i] = val.to_pydatetime()\n",
    "# print(time_list)\n",
    "\n",
    "# dotw = time_list[0].strftime(\"%A\")\n",
    "# print(dotw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e730e5d7-ef1d-4383-aa9c-66f7b7d2d351",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dotw analysis"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import webbrowser as wb\n",
    "\n",
    "top_num = 1000\n",
    "\n",
    "installation = \"a037a00000pwUFlAAM\"\n",
    "base_path = '/dbfs/FileStore/tables/dotw/'\n",
    "read_filepath = base_path + installation + \".csv\"\n",
    "df_dotw = pd.read_csv(read_filepath).drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df_bins = pd.DataFrame({'BinNo' : df_dotw['BinNo']}).drop_duplicates()\n",
    "# display(df_bins.head(top_num))\n",
    "\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df_days = pd.DataFrame({'dotw' : day_order})\n",
    "\n",
    "df_all_combos = df_days.merge(df_bins, how='cross')\n",
    "# display(df_bins.head(top_num))\n",
    "\n",
    "df_dotw = df_all_combos.merge(df_dotw, on=['dotw', 'BinNo'], how='left').fillna(0)\n",
    "df_dotw['count'] = df_dotw['count'].astype(int)\n",
    "df_dotw['dotw'] = pd.Categorical(df_dotw['dotw'], categories=day_order, ordered=True)\n",
    "df_dotw = df_dotw.sort_values(['dotw','count'], ascending=[True, False])\n",
    "# display(df_dotw.head(top_num))\n",
    "\n",
    "# rand = random.randint(0, len(df_bins['BinNo']) - top_num)\n",
    "# df_test = df_dotw[df_dotw['BinNo'].isin(df_bins['BinNo'][rand:rand+top_num])].copy()\n",
    "\n",
    "df_sum = df_dotw.groupby(['BinNo']).sum('count') \\\n",
    "                .sort_values(['count'], ascending=False) \\\n",
    "                .reset_index()\n",
    "df_top = df_sum.drop('count', axis=1) \\\n",
    "                .head(top_num) \\\n",
    "                .merge(df_dotw, on='BinNo', how='left')\n",
    "\n",
    "# display(df_sum)\n",
    "# display(df_top)\n",
    "\n",
    "df_mon = df_dotw[df_dotw['dotw'] == 'Monday'].rename(columns={'count':'count_mon'}).drop('dotw', axis=1).copy()\n",
    "df_tue = df_dotw[df_dotw['dotw'] == 'Tuesday'].rename(columns={'count':'count_tue'}).drop('dotw', axis=1).copy()\n",
    "df_wed = df_dotw[df_dotw['dotw'] == 'Wednesday'].rename(columns={'count':'count_wed'}).drop('dotw', axis=1).copy()\n",
    "df_thu = df_dotw[df_dotw['dotw'] == 'Thursday'].rename(columns={'count':'count_thu'}).drop('dotw', axis=1).copy()\n",
    "df_fri = df_dotw[df_dotw['dotw'] == 'Friday'].rename(columns={'count':'count_fri'}).drop('dotw', axis=1).copy()\n",
    "df_sat = df_dotw[df_dotw['dotw'] == 'Saturday'].rename(columns={'count':'count_sat'}).drop('dotw', axis=1).copy()\n",
    "df_sun = df_dotw[df_dotw['dotw'] == 'Sunday'].rename(columns={'count':'count_sun'}).drop('dotw', axis=1).copy()\n",
    "\n",
    "df_join = pd.merge(df_mon, df_tue, on='BinNo', how='outer') \\\n",
    "            .merge(df_wed, on='BinNo', how='outer') \\\n",
    "            .merge(df_thu, on='BinNo', how='outer') \\\n",
    "            .merge(df_fri, on='BinNo', how='outer') \\\n",
    "            .merge(df_sat, on='BinNo', how='outer') \\\n",
    "            .merge(df_sun, on='BinNo', how='outer') \\\n",
    "            .merge(df_sum, on='BinNo') \\\n",
    "            .rename(columns={'count':'count_tot'}) \\\n",
    "            .sort_values(by=['count_tot'], ascending=False)\n",
    "\n",
    "mean_list = df_join.head(top_num).mean().drop(['BinNo', 'count_tot']).tolist()\n",
    "std_list = df_join.head(top_num).std().drop(['BinNo', 'count_tot']).tolist()\n",
    "df_stats = pd.DataFrame({'dotw' : day_order, 'mean' : mean_list, 'std' : std_list})\n",
    "\n",
    "# display(df_join.head(top_num))\n",
    "# display(df_stats)\n",
    "\n",
    "# fig = px.line(df_top, x='dotw', y='count', color='BinNo', markers=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for bin_no in df_top['BinNo'].unique():\n",
    "    df_bin = df_top[df_top['BinNo'] == bin_no]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_bin['dotw'],\n",
    "        y=df_bin['count'],\n",
    "        name=f'Bin {bin_no}',\n",
    "        mode='lines+markers',\n",
    "        marker=dict(symbol='circle'),\n",
    "        line=dict(width=1)\n",
    "    ))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_stats['dotw'],\n",
    "    y=df_stats['mean'],\n",
    "    name='MEAN+STD',\n",
    "    mode='markers',\n",
    "    marker=dict(color='black', symbol='diamond', size=10),\n",
    "    legendrank=1,\n",
    "    error_y=dict(\n",
    "        type='data',\n",
    "        array=2.5 * df_stats['std'],\n",
    "        visible=True\n",
    "    )\n",
    "))\n",
    "fig.show()\n",
    "display(df_join.head(top_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa91140-7df6-4771-85ac-831287484b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_cb_content = df.select(['local_installation_timestamp', 'tag', 'values'])\n",
    "data = tuple((row['content_code'], row['count']) for row in df_cb_content.collect())\n",
    "print(len(data))\n",
    "if len(data) == 0:\n",
    "    dbutils.notebook.exit(\"no data\")\n",
    "    \n",
    "content_code, count = zip(*data)\n",
    "# print(type(data[0][0]))\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(data)\n",
    "# print(kmeans.labels_)\n",
    "centers = kmeans.cluster_centers_\n",
    "grouping = kmeans.labels_\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].scatter(content_code, count, c=kmeans.labels_, s=10)\n",
    "axs[0].scatter(centers[:, 0], centers[:, 1], c='red', s=20, alpha=0.5, marker='X')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xlabel('content code')\n",
    "axs[0].set_ylabel('count')\n",
    "axs[0].set_ylim(0, 10e8)\n",
    "axs[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "axs[1].scatter(centers[:, 0], centers[:, 1], c='red', s=20, alpha=0.5, marker='X')\n",
    "axs[1].set_xlabel('content code')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_ylim(0, 10e8)\n",
    "axs[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "high_throughput = []\n",
    "for i in range(0, len(content_code)):\n",
    "    if grouping[i] == 1:\n",
    "        high_throughput.append(content_code[i])\n",
    "print('content_code:', content_code)\n",
    "print('count:', count)\n",
    "print('groupings:', grouping)\n",
    "print('high throughput content codes:', high_throughput)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6275803671446566,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Unify Data Testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
